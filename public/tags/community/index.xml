<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Community on Nandaja.</title>
    <link>http://localhost:1313/tags/community/</link>
    <description>Recent content in Community on Nandaja.</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>cn</language>
    <lastBuildDate>Sat, 19 Sep 2015 18:58:42 +0530</lastBuildDate>
    
	<atom:link href="http://localhost:1313/tags/community/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>From Future Import PeARS, Part - 1</title>
      <link>http://localhost:1313/post/2015-09-18-from-future-import-pears-part-1.markdown/</link>
      <pubDate>Sat, 19 Sep 2015 18:58:42 +0530</pubDate>
      
      <guid>http://localhost:1313/post/2015-09-18-from-future-import-pears-part-1.markdown/</guid>
      <description>&lt;p&gt;PeARS (or Peer-to-peer Agent for Reciprocated Search, if you would prefer that) is a distributed search engine project that I got involved with very recently. It is a very interesting and ambitious idea that &lt;a href=&#34;http://aurelieherbelot.net/&#34;&gt;Aurelie Herbelot&lt;/a&gt; thought of, to make searching more secure, accurate and very much user reliant. For an official intro from the skipper herself head over to her &lt;a href=&#34;http://aurelieherbelot.net/pears/&#34;&gt;blog&lt;/a&gt;. What I am trying to share via this post is the current status of the project, where we see PeARS down the road, how much we would love others to look into the project, and what not. Hopefully the first of many more come. :-)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/pear-logo.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;###What is PeARS?&lt;/p&gt;
&lt;p&gt;Today, we have innumerable people using the Internet, each one of whom has access to an enormous amount of data. We are at a point of time where the zillion Internet users can help each other out to find information we are looking for rather than depending on centralized search engines some big corporate has set up for us. That&amp;rsquo;s exactly the idea behind PeARS. Say n number of people have installed PeARS in their local machine. Over time, PeARS will index the search history of your browser(This happens locally and also you can choose what to index). Each one of these pears, that&amp;rsquo;s what we call each installed system, will have a unique profile(anonymous or other wise based on the user preference) identifiable by another pear and can share their search index which other pears can use.&lt;/p&gt;
&lt;p&gt;For example, Han is a skilled pilot who reads a lot of super awesome technical stuff about spaceships. He decides to index most of his browser history using PeARS and share it with other pears anonymously(he is trying to keep a low profile since Jabba the Hutt is looking for him). Luke, who just installed PeARS in his desktop, queries about Millennium Falcon. The distributional semantics algorithms in PeARS get search indices from pears which has same or similar terms. It clearly identifies Han Solo as a spaceship expert, gets his index along with others&amp;rsquo; and present it to Luke the fastest way possible. Luke starts reading about spaceships. He is not a bad pilot himself, now. :)&lt;br/&gt;
Luke try looking up about coolest lightsabers, but unfortunately none of the jedis were using PeARS. He gets redirected to Duck Duck go for the results. Over time, his search history also gets indexed locally which he decides to share. He became the most resourceful pear on lightsabers. And it goes on&amp;hellip;&lt;/p&gt;
&lt;p&gt;So like I mentioned earlier, it is very much reliant on the users of the big fat Internet. The links shared by each user helps PeARS rate them based on their resourcefulness on each topic and help others get most accurate results for their queries. No one tracking your search queries and no corporate telling you what to see.&lt;/p&gt;
&lt;p&gt;It may sound incredibly ambitious, but the data each single one of us has access to makes this idea very much possible.&lt;/p&gt;
&lt;p&gt;###Present Status&lt;/p&gt;
&lt;p&gt;We are very much in the initial phase of the project. We have a prototype working which can index your local searches and queries from the couple of pears we have setup in our machines or from your local index.&lt;/p&gt;
&lt;p&gt;###What next?&lt;/p&gt;
&lt;p&gt;Most of the work is yet to be done. Presently, the system is not that efficient when it comes to performance, although search accuracy is near perfect. We are working on changing the data structures being used and making PeARS work faster. We plan an implementation very similar to BitTorrent to track and share data among the pears. We are yet to come up with the architecture.&lt;/p&gt;
&lt;p&gt;###Who are we?&lt;/p&gt;
&lt;p&gt;We are a bunch of people, who strongly believe that the future is distributed, working together to make a small step towards the next big change. The 5 of us involved with this project are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://aurelieherbelot.net/&#34;&gt;Aurelie Herbelot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://stultus.in&#34;&gt;Hrishikesh K.B.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Veesa Norman&lt;/li&gt;
&lt;li&gt;Shobha Tyagi
&lt;br/&gt;and of course me. :)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We usually hangout at the IRC channel #pears in Freenode.&lt;/p&gt;
&lt;p&gt;###Do we need help?&lt;/p&gt;
&lt;p&gt;What kind of question is that? Of course, we do!
&lt;br/&gt;The programming language we use is Python. We have hosted our code in Github, you can find the link below. We would be very much grateful to have people help us with developing, testing, documenting, publicizing, etc.&lt;/p&gt;
&lt;p&gt;More details about the project to be followed in the next post.&lt;/p&gt;
&lt;p&gt;###Some links&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. &lt;a href=&#34;https://github.com/minimalparts/PeARS&#34;&gt;code&lt;/a&gt;&lt;/strong&gt;
&lt;br/&gt;&lt;strong&gt;2. &lt;a href=&#34;http://aurelieherbelot.net/pears/&#34;&gt;More about PeARS&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GSoC Weekly update</title>
      <link>http://localhost:1313/post/2013-08-26-gsoc-weeklystatus-update-9/</link>
      <pubDate>Mon, 26 Aug 2013 19:58:42 +0530</pubDate>
      
      <guid>http://localhost:1313/post/2013-08-26-gsoc-weeklystatus-update-9/</guid>
      <description>&lt;p&gt;The work of mine has been correcting the reference glyph files and developing a web interface for the proposed framework. I had tried and made the reference files least buggy as possible. I have gone through the glyph names of almost all the 243 words in 4 fonts. I had to invest a lot of time on this especially due to one minor misunderstanding of mine on the multiple correct renderings of the words. And I hope it will get much refined after Rajeeshettan proof read it for 2 fonts as he has suggested.&lt;br&gt;
(I have changed the renderings of words with repham in Rachana such that the dotreph comes first. So words like these &lt;a href=&#34;http://troll.ws/image/2e3a872e&#34;&gt;http://troll.ws/image/2e3a872e&lt;/a&gt;, &lt;a href=&#34;http://troll.ws/image/469dd87a&#34;&gt;http://troll.ws/image/469dd87a&lt;/a&gt;, &lt;a href=&#34;http://troll.ws/image/5838dbec&#34;&gt;http://troll.ws/image/5838dbec&lt;/a&gt; although looks correct, will be in the wrongly rendered words list by harfbuzz.)&lt;/p&gt;
&lt;p&gt;The next part of this weeks work was developing the web interface (Excuse my poor design, I am cleaning it up as I write). It doesn&amp;rsquo;t actually spits output to the user now or doesn&amp;rsquo;t make it easier for the user to open files. I am hoping to make it run the script well in a week&amp;rsquo;s time and don&amp;rsquo;t think it is ready yet for the review. So I would like another week to make it ready for reviewing.&lt;/p&gt;
&lt;p&gt;And finally about the C code I have added to the repo. I will start working on a new code in C++ once I am done with the webpage as I find the present code massively buggy and really inefficient. I hope I&amp;rsquo;ll be able to update it the week after next.&lt;/p&gt;
&lt;p&gt;My code here: &lt;a href=&#34;https://github.com/nandajavarma/Automated-Rendering-Testing&#34;&gt;https://github.com/nandajavarma/Automated-Rendering-Testing&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GSoC Weekly update</title>
      <link>http://localhost:1313/post/2013-08-17-gsoc-weekly-status-update-8/</link>
      <pubDate>Sat, 17 Aug 2013 18:58:42 +0530</pubDate>
      
      <guid>http://localhost:1313/post/2013-08-17-gsoc-weekly-status-update-8/</guid>
      <description>&lt;p&gt;I have changed the framework interface from its previous form, although the previous front end automated_rendering_testing.py is still present in the repo. Now the new interface, rendering_testing.py, need all the file names to be provided as command line arguments. The user gets the convenience  of using the tab completion this way. The user will have to give as command line arguments 6 files (font  file, test cases file, reference file, rendering output and files to store output) and an optional directory name(if the engine is harfbuzz).&lt;/p&gt;
&lt;p&gt;If the rendering engine is harfbuzz, user can run the script generate_hb_rendering.py  along with the test cases file and font file as parameters, to create the rendered output file. If that is not the case, the user will have to create this file as well in the prescribed form.&lt;/p&gt;
&lt;p&gt;Now, the algorithm that actually test the rendering was a bit buggy and was giving certain wrong outputs for words with multiple rendering engines and I have cleared this error. This feature gives correct output now for the files I tried it with.&lt;/p&gt;
&lt;p&gt;The next thing I am working on is the web interface and I am using Flask framework. Will make this code public as soon as I get the script running from the page.&lt;/p&gt;
&lt;p&gt;Find the code here: &lt;a href=&#34;https://github.com/nandajavarma/Automated-Rendering-Testing&#34;&gt;https://github.com/nandajavarma/Automated-Rendering-Testing&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GSoC Weekly update</title>
      <link>http://localhost:1313/post/2013-08-11-gsoc-weekly-update-6/</link>
      <pubDate>Sun, 11 Aug 2013 18:58:42 +0530</pubDate>
      
      <guid>http://localhost:1313/post/2013-08-11-gsoc-weekly-update-6/</guid>
      <description>&lt;p&gt;The past two weeks has been a blur with a lot of travelling and minimal Internet access. The following are the works I have been doing so far:&lt;/p&gt;
&lt;p&gt;The following modifications were asked to be made on the existing framework by my mentor after a Hangout session as part of the evaluations:&lt;/p&gt;
&lt;p&gt;1. Modify the comparison algorithm so as to show positive results for the words with multiple correct renderings - This modification is made. Now, the user can give multiple glyph names separated by comma in the reference file and if the rendering matches any one of these, the framework will return a positive response.&lt;/p&gt;
&lt;p&gt;2. Modify the reference glyph file, adding the glyph names of words with multiple correct renderings. Also some corrections were asked to be made in the existing reference file.&lt;/p&gt;
&lt;p&gt;3. Modify the framework such that the user can even test by giving the file names as parameters. This one needs a little more work as I didn&amp;rsquo;t give options in argument parser for all the necessary file inputs. Will update this soon.&lt;/p&gt;
&lt;p&gt;Along with these some minor fixes were asked to be done on the script and all those are taken care of.&lt;/p&gt;
&lt;p&gt;As for the further developments, planned to create a web interface for this framework. I am trying to create this interface using Flask and I am currently working on it.&lt;/p&gt;
&lt;p&gt;After that, the framework will be implemented in C. I have added a partially working implementation of this in the repo.&lt;/p&gt;
&lt;p&gt;After the completion of all these, if time permits, references for other fonts are also planned to be made.&lt;/p&gt;
&lt;p&gt;Will keep posted on further developments.&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;p&gt;Find my code here: &lt;a href=&#34;https://github.com/nandajavarma/Automated-Rendering-Testing&#34;&gt;https://github.com/nandajavarma/Automated-Rendering-Testing&lt;/a&gt;&lt;a href=&#34;https://gitlab.com/gem/automated-rendering-testing/tree/master&#34;&gt;&lt;br&gt;
&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GSoC Weekly update</title>
      <link>http://localhost:1313/post/2013-07-28-gsoc-weekly-update-5/</link>
      <pubDate>Sun, 28 Jul 2013 15:58:42 +0530</pubDate>
      
      <guid>http://localhost:1313/post/2013-07-28-gsoc-weekly-update-5/</guid>
      <description>&lt;p&gt;The works this week has been a little slow with college exams and assignments. This is what I have done so far this week.&lt;/p&gt;
&lt;p&gt;I have completed the list of reference files containing glyph names of 243 words from four fonts each. Fonts being: Rachana, Meera, Suruma and Lohit-Malayalaam.&lt;/p&gt;
&lt;p&gt;The code has been modified to equip not only harfbuzz renderings but renderings from other engines line Uniscribe, provided the user will produce the output of the rendering engine herself/himself. I have created a Python package containing 2 modules each for testing and creating output. The main script automated_rendering_testing.py  will make use of this package to test and give the final result. To test the framework, one can just run ./automated_rendering_testing and then provide the necessary  information, when asked.&lt;/p&gt;
&lt;p&gt;Coming to the tester, first it will compare the reference file and the rendering output. The it will create a file named result.txt containing the wrongly rendered words along with the number corresponding to the word in test cases&amp;rsquo; file. This file is used only to create the png file of the wrongly rendered words, if the engine is harfbuzz. Other wise this file is ignored. Now the actual output is a file test_result.txt with the format:&lt;/p&gt;
&lt;p&gt;Sl.No Word Rendering status(correct/wrong)&lt;/p&gt;
&lt;p&gt;User can view this file, see the status and see the wrongly rendered word.&lt;/p&gt;
&lt;p&gt;The framework works this way now: &lt;a href=&#34;http://nandajavarma.files.wordpress.com/2013/07/screenshot-from-2013-07-28-155652.png&#34;&gt;![Image]({{ site.baseurl }}/assets/screenshot-from-2013-07-28-155652.png?w=650)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And this would be the output.png file. (As I chose harfbuzz here)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nandajavarma.files.wordpress.com/2013/07/screenshot-from-2013-07-28-155950.png&#34;&gt;![Image]({{ site.baseurl }}/assets/screenshot-from-2013-07-28-155950.png?w=158)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And the test_result.txt file would look like this:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nandajavarma.files.wordpress.com/2013/07/screenshot-from-2013-07-28-160136.png&#34;&gt;![Image]({{ site.baseurl }}/assets/screenshot-from-2013-07-28-160136.png?w=569)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The agenda for this week is to re-write the whole code in C.&lt;/p&gt;
&lt;p&gt;My code is available here: &lt;a href=&#34;https://github.com/nandajavarma/Automated-Rendering-Testing&#34;&gt;https://github.com/nandajavarma/Automated-Rendering-Testing&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GSoC Weekly update</title>
      <link>http://localhost:1313/post/2013-07-20-gsoc-weekly-report-4/</link>
      <pubDate>Sat, 20 Jul 2013 19:58:42 +0530</pubDate>
      
      <guid>http://localhost:1313/post/2013-07-20-gsoc-weekly-report-4/</guid>
      <description>&lt;p&gt;This week my main task was to migrate my code to Python. As of now I have implemented my algorithm in Python. Here is the link to the repo : &lt;a href=&#34;https://github.com/nandajavarma/Automated-Rendering-Testing&#34;&gt;https://github.com/nandajavarma/Automated-Rendering-Testing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I have expanded my test cases&amp;rsquo; list a bit. Now it has 243 Malayalam words. I have manually created files with glyph names of these test cases in four fonts: Rachana, Meera, Suruma and Lohith-Malayalam in files names rachana-glyph.txt,  meera-glyph.txt etc. (It is still a bit buggy, so haven&amp;rsquo;t pushed the latest commit of this yet).&lt;/p&gt;
&lt;p&gt;What the code basically does is, it will ask the tester which font she/he wants to test in. Say it is Meera. The code will look for the reference file which we manually create and the file with harfbuzz rendering of the test cases, names as hb_meera_rendering.txt. This file can be created by running harfbuzzrendering.py script with proper font files in the current directory. The main script rendering_testing.py will scan both these files and compare the glyph name corresponding to each word and stores the wrongly rendered words to a new list. Finally hb-view will be executed on the words inside this list and a file named output.png will be generated in the same directory that contains all the wrongly rendered words.&lt;/p&gt;
&lt;p&gt;The baseline glyph names&amp;rsquo; files aren&amp;rsquo;t ready yet with complete glyph names of all the 243 words. Will be able to complete it within 1-2 days.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GSoC Weekly update</title>
      <link>http://localhost:1313/post/2013-07-14-gsoc-weekly-update-3/</link>
      <pubDate>Sun, 14 Jul 2013 19:58:42 +0530</pubDate>
      
      <guid>http://localhost:1313/post/2013-07-14-gsoc-weekly-update-3/</guid>
      <description>&lt;p&gt;This week I&amp;rsquo;ve been working on generating a baseline glyphs file for 4 fonts: Rachana, Meera, Suruma and Lohith-Malayalam. I have selected some malayalam words from harfbuzz tree and Santhosh Thottingal&amp;rsquo;s test cases which I thought would be enough to test rendering problems. Then I started listing the glyph names of these files for each fonts in separate text files. To get the corresponding Unicode code point of each word, I wrote a small Java code. So I executed the script on each word, found all the code points and made 4 text files that contains the corresponding glyph names of the four fonts I mentioned earlier.&lt;/p&gt;
&lt;p&gt;Although my mentor did tell me that it is not possible to generate glyph names automatically, I wasted more than a couple of days on a Font Forge script to make it automatically output the glyph names. But that gives the glyph name only if we click on each character, which became terribly disappointing. So instead I used it to make the baseline glyphs file in the structure I want if I click on the necessary characters. But this code is trivial as far as rendering testing is concerned and will leave it out from now (Just noting it down as it wasted a very non-trivial amount of my time ;-) ).&lt;/p&gt;
&lt;p&gt;I have modified the main C code such that it will ask the tester which font she wants and after choosing the one she needs it will output the result based on the words I have given.&lt;/p&gt;
&lt;p&gt;But my mentor pointed out that it looks quite messy looking at codes in 3 different languages for a single framework so I&amp;rsquo;ll be re-writing my code in Python this week.&lt;/p&gt;
&lt;p&gt;You can find my code here: &lt;a href=&#34;https://github.com/nandajavarma/Automated-Rendering-Testing&#34;&gt;https://github.com/nandajavarma/Automated-Rendering-Testing&lt;/a&gt; (although the README is not up-to-date)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GSoC Weekly update</title>
      <link>http://localhost:1313/post/2013-06-29-gsoc-weekly-update-2/</link>
      <pubDate>Sat, 29 Jun 2013 15:58:42 +0530</pubDate>
      
      <guid>http://localhost:1313/post/2013-06-29-gsoc-weekly-update-2/</guid>
      <description>&lt;p&gt;Coding period for GSoC has started the past week and I have been working on a very simple implementation of the proposal in C and two tiny bash scripts. My code is available here: &lt;a href=&#34;https://github.com/nandajavarma/Automated-Rendering-Testing&#34;&gt;https://github.com/nandajavarma/Automated-Rendering-Testing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first thing to be done to test using these scripts is create a file that contains a set of words to be tested to see if their rendering is correct. Here I have taken a sample test data file created by SMC a while ago (ml-harfbuzz-testdata,txt). Now pass this file through the script render_test.sh along with the necessary font file. That is:&lt;/p&gt;
&lt;p&gt;./render_test.sh ml-harfbuzz-testdata.txt /path/to/fontfile&lt;/p&gt;
&lt;p&gt;This will create a file named rendered_glyphs.txt that contains the output of hb-shape function of harfbuzz, i.e. the glyph name followed by some additional numbers (which will be ignored for now).&lt;/p&gt;
&lt;p&gt;Now create a file that contains the actual glyph names of the words in the the test data wordfile. I got the data from font forge. This has to be created manually and, as of now, obeying the following structure:&lt;/p&gt;
&lt;p&gt;[glyph11,glyph12,glyph13,&amp;hellip;,glyph1n]&lt;/p&gt;
&lt;p&gt;[glyph21,glyph22,glyph33,&amp;hellip;.,glyph2n]&lt;/p&gt;
&lt;p&gt;.&lt;/p&gt;
&lt;p&gt;.&lt;/p&gt;
&lt;p&gt;.&lt;/p&gt;
&lt;p&gt;Also make sure that glyph names of each word is in the same order as that of the corresponding words in the test data file. I have named it orig_glyphs.txt Once this is done, we can pass the above two files through the executable of the script rendering_testing.c, say rendering_testing. That is:&lt;/p&gt;
&lt;p&gt;./rendering_testing orig_glyphs.txt rendered_glyphs.txt&lt;/p&gt;
&lt;p&gt;This script will compare the glyphs in order and if it find any pairs that doesn&amp;rsquo;t match, it will write to a file, result.txt, the line number in which the word appears in the test data file. Otherwise it will tell you the renderings are perfect.&lt;/p&gt;
&lt;p&gt;Once this is done, to see the words with wrong renderings we will have to run the third script show_rendering.sh. It takes as input the result.txt file, the test data file and also the font file. That is:&lt;/p&gt;
&lt;p&gt;./show_rendering.sh result.txt ml-harfbuzz-testdata.txt /path/to/fontfile&lt;/p&gt;
&lt;p&gt;This script will create png images of the wrongly rendered words in the current directory.&lt;/p&gt;
&lt;p&gt;That is all about my scripts. But the C code is very much inefficient. It even spits segmentation faults with some files. Once I make sure that I am on the right path after discussing with my mentor, I will be working on improving my algorithm and making this code better. That would be my next week&amp;rsquo;s work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GSoC - Community engagement period</title>
      <link>http://localhost:1313/post/2013-06-10-gsoc-community-engagement-period/</link>
      <pubDate>Mon, 10 Jun 2013 18:58:42 +0530</pubDate>
      
      <guid>http://localhost:1313/post/2013-06-10-gsoc-community-engagement-period/</guid>
      <description>&lt;p&gt;GSoC 2013 approved project list was published on May 27th and the community engagement period was started from May 29th onwards. During this period the students are supposed to bond with their mentors, read the documentations and finalize your plans so you can have a head start with your project. The project topic for which I have got accepted for is &amp;ldquo;Automated rendering testing&amp;rdquo; and I will be completing that project under Swathanthra Malayalam Computing. I could learn a lot a new stuff so far during this community bonding period with a heavy deal of help from my mentor Rajeesh K Nambiar, although I haven&amp;rsquo;t started actual coding yet. I will try to explain my proposal status and further steps here, in detail.&lt;/p&gt;
&lt;p&gt;Basically, my project idea is to create an automated way to test the rendering of Indic fonts by rendering engines like harfbuzz. The procedure I wish to follow here is quite simple. Create a test file that contains a set of words, mostly characters with ligatures that will be used for testing the rendering. Along with that I will be maintaining a file that contain the glyph infos of the words/characters in the test file for a particular font, say Malayalam font Rachana. As of now I am preparing it manually, can switch to font forge scripts if required.&lt;/p&gt;
&lt;p&gt;Once I have got all the test data, my main script will accept the entries in the test file and render it using Harfbuzz for the font Rachana. The words will be rendered using hb-shape and the output glyph values will be compared with the original glyph indices of these words that I have collected manually. If the glyph indices doesn&amp;rsquo;t match, an error flag will be set for that particular word. At the end of the comparisons, the words with error flag set can be rendered using hb-view and stored in another html file. This file can be looked up to see for rendering issues.&lt;/p&gt;
&lt;p&gt;This is what I will be implementing first. Depending on its efficiency, will move to any other solutions. In the above procedure, the most inefficient step, I think, is collecting the test file step and collecting the glyph index step. We can resolve the latter by, may be, using a scripts for extraction or using the .ttx file of the font (which is quite complex). But the former is a real issue. If the user wants to check for rendering issues in a font, she will have to create this file with a set of words manually. Will have to think of a way to overcome this issue.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s it for now!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MiniDebConf at NIT-C</title>
      <link>http://localhost:1313/post/2013-03-11-minidebconf-at-nit-c/</link>
      <pubDate>Mon, 11 Mar 2013 19:58:42 +0530</pubDate>
      
      <guid>http://localhost:1313/post/2013-03-11-minidebconf-at-nit-c/</guid>
      <description>&lt;p&gt;Ever since I started loving FOSS, I really wanted to attend a FOSSMeet. And last month I got extremely lucky and could attend FOSSMeet at NIT-C. This year along with FOSSMeet a MiniDebConf was also arranged. A month prior to it, I was asked to give a Ruby gem packaging session at this MiniDebConf. I was quite nervous because this was the first FOSS event I ever attended and I was asked to give a session. But I knew it would be a great experience, So I decided to go with it.&lt;/p&gt;
&lt;p&gt;FOSSMeet was conducted from 22nd to 24th of February and my session was scheduled for 24th. I reached Calicut town on 23rd morning (Obviously unprepared) and when I was just about to hop into a bus to NIT, one of the MiniDebConf organizers, &lt;a href=&#34;http://swvist.github.com/&#34;&gt;Vipin&lt;/a&gt; called me and said that my session was preponed to that day afternoon. I started freaking out because I still had to prepare slides and decide what all to include in my session. As soon as I reached NIT, I had to go to my relatives&amp;rsquo; place at the NIT apartments, where I was supposed to stay for the next two days ( I preferred hostel, but whatever!). After that I rushed to NIT campus, got my speaker&amp;rsquo;s ID card from the registration counter and started roaming around the campus searching for the MiniDebConf venue. Finally I found that it was going on at the networks lab and got there. When I entered &lt;a href=&#34;http://look-pavi.blogspot.in/&#34;&gt;Pavi&lt;/a&gt; and &lt;a href=&#34;http://flossexperiences.wordpress.com/&#34;&gt;Shirish&lt;/a&gt; were giving talks on debian releases and stuff and I quite honestly didn&amp;rsquo;t know how to introduce myself. &amp;lsquo;Hey, I am Nandaja. I am a speaker here&amp;rsquo; didn&amp;rsquo;t sound polite enough. So I followed a guy, who assigned me a system in the lab thinking I am a delegate there. So I sat there and started preparing slides for my session until the lunch break.  Once I reached the hall were lunch was being served, I decided to socialize. I met some cool SMC people who were really easy going and friendly and then I met &lt;a href=&#34;http://www.j4v4m4n.in/&#34;&gt;Praveen&lt;/a&gt;, whom I was more than happy to meet. After the lunch, we went back to the NSL lab and there Praveen introduced me to Shirish and Pavi, two Debian gurus. It was the time for Debian installation workshop. Pavi asked me to help the delegates with the partitioning and stuff, which I tried doing and I don&amp;rsquo;t think the students who listened understood much. We clapped for the teams who successfully completed the installation. The session lasted till evening and it was supper time soon after. So my packaging session was again rescheduled for the next day. I again met some cool geeks there. There was a really nice Aaron Swartz memorial talk by Vishnu later that night. Following that was hack night and a talk by Praveen on open source contributions. It was getting too late and had to get back to the relatives&amp;rsquo; place. So I missed it (The reason why I preferred hostel).&lt;/p&gt;
&lt;p&gt;So it was day-2 and my session was scheduled as the first event at MiniDebConf, that day. I got into NSL and heard all of them had real fun last night, albeit it wasn&amp;rsquo;t that much of a &amp;lsquo;hack&amp;rsquo; night. It was supposed to be a full day packaging session at MiniDebConf, which me and Praveen decided to do together. But as many other interesting events were happening simultaneously, only very few came. So we decided we&amp;rsquo;ll start packaging session once all other interesting events are over. But unfortunately, the students who showed up started leaving one by one. Luckily for me, a very enthusiastic girl showed up, who missed first day&amp;rsquo;s session and wanted to learn more about Debian. I started giving a private session for her. But around half an hour later, she said she has got a farewell party to attend and left (Yeah! She literally ran away :-P ). Till the lunch time we sat there chit-chatting. We even went to the 3D-Blender workshop but as soon as the speaker&amp;rsquo;s lap screen showed a windows desktop we left (Come on, It&amp;rsquo;s FOSSMeet, man!). After lunch, we exactly had 6 people at the packaging session and we decided to do it anyway. &lt;a href=&#34;http://stultus.in/&#34;&gt;Hrishi&lt;/a&gt; started off the session with an introduction to diaspora. Then me and Praveen together started explaining packaging. There were some really cool people there, like, Vamsee Kanakala, the Executive developer at Bang The Table, a government employee, whose sole intention was to attend the packaging event and four other enthusiastic students. The session proceeded pretty well and they were picking it up pretty fast. Almost by the end of the session, Vamsee asked me if I would like to do an internship at Bang The Table and I accepted the offer right away. :-) We couldn&amp;rsquo;t cover the whole packaging process because it was time for the closing ceremony. Anish, Praveen Vamsee and many others gave the feedback. Vamsee was really kind and he even mentioned my name at the feedback session. I was also asked to give the feedback and honestly I don&amp;rsquo;t remember what the heck I said there. All of us speakers got a beautiful momento. I bought a FOSSMeet t-shirt and was about to leave.&lt;/p&gt;
&lt;p&gt;Just then, Vipin, &lt;a href=&#34;http://jaseemabid.github.com/&#34;&gt;Jaseem&lt;/a&gt;, Pavi, Praveen, Shirish, &lt;a href=&#34;https://joindiaspora.com/people/37c67c47e1b65ee1&#34;&gt;Akshat&lt;/a&gt;, &lt;a href=&#34;http://thecodecracker.com/&#34;&gt;Jishnu&lt;/a&gt; and &lt;a href=&#34;https://joindiaspora.com/u/anish&#34;&gt;Anish&lt;/a&gt; were planning to have a group dinner and invited me as well. It was pretty cool. I never though I would have so much fun hanging out with such uber geeks. After that, it was time for us to bid farewell and all of us went on our own ways.&lt;/p&gt;
&lt;p&gt;As it was late, I couldn&amp;rsquo;t go back home that day. I left Calicut the day after. Quite frankly, I was pretty sad that the FOSSMeet was over. But I was really happy too at the same time, because I made some new geeky friends, got an internship offer at Bangalore and had a lot of fun. Plans for the next MiniDebConf were going on. I am so looking forward to attend that one and hope it happens soon. :-)&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>